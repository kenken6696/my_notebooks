{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Linear Regression  \n",
    "    回帰\n",
    "    単回帰:独立な説明変数が1つ  \n",
    "    重回帰:独立な説明変数が2つ以上  \n",
    "    多項式回帰:独立な説明変数は1つだが、べき乗で表される  \n",
    "    - 方法\n",
    "    - 得意領域\n",
    "    - 注意点  \n",
    "    Anscom's quartet⇒可視化大事  \n",
    "    線型回帰は同じとなるが4つの異なるデータパターンがある\n",
    "        1. 普通\n",
    "        2. 曲線\n",
    "        3. 外れ値\n",
    "        4. 線形でないデータの外れ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = [[10.0], [8.0], [13.0], [9.0], [11.0], [14.0], [6.0], [4.0], [12.0], [7.0], [5.0]]\n",
    "y = [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68]\n",
    "model = LinearRegression()\n",
    "model.fit(X, y) \n",
    "print(model.intercept_) # 切片 \n",
    "print(model.coef_) # 傾き\n",
    "y_pred = model.predict([[0], [1]]) \n",
    "print(y_pred) # x=0, x=1に対する予測結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Regularization  \n",
    "    回帰、過学習防止のため誤差関数に罰則項を設ける\n",
    "    - 方法  \n",
    "        誤差関数=損失関数+罰則項(正則化項)\n",
    "        - Rigde:罰則項=α×学習パラメータの二乗和\n",
    "        $$\n",
    "        α(w_1^2+w_2^2)\n",
    "        $$\n",
    "        - Lasso:罰則項=α×学習パラメータの絶対値の和\n",
    "        $$\n",
    "        α(|w_1|+|w_2|)\n",
    "        $$\n",
    "    - 得意領域\n",
    "    - 注意点\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Logistic Regression  \n",
    "    分類、連続値から二値分類する、シグモイド関数\n",
    "    - 方法\n",
    "    - 得意領域\n",
    "    - 注意点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train = np.r_[np.random.normal(3, 1, size=50), np.random.normal(-1, 1, size=50)].reshape((100, -1))\n",
    "y_train = np.r_[np.ones(50), np.zeros(50)]\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "model.predict_proba([[0], [1], [2]])[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. SVM(Linear Support Vector Machine)  \n",
    "    分類/回帰\n",
    "    - 方法  \n",
    "        マージンの最大化  \n",
    "        マージン:決定領域と各クラスの最も近いデータ値の距離の差\n",
    "        - ハードマージン:マージン内にデータを許容しない\n",
    "        - ソフトマージン:マージン内にデータを許容する\n",
    "    - 得意領域\n",
    "    - 注意点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# データ生成\n",
    "centers = [(-1, -0.125), (0.5, 0.5)]\n",
    "X, y = make_blobs(n_samples=50, n_features=2, centers=centers, cluster_std=0.3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "model = LinearSVC() \n",
    "model.fit(X_train, y_train) # 学習\n",
    "y_pred = model.predict(X_test) \n",
    "accuracy_score(y_pred, y_test) # 評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Support Vector Machine(kernel method)  \n",
    "    分類、kernel法版SVM  \n",
    "    - 方法  \n",
    "        決定領域を決められないデータを高次元化して分類する。  \n",
    "        カーネル関数には、線形カーネル、シグモイドカーネル、多項カーネル、RBFカーネルなどがある。\n",
    "    - 得意領域\n",
    "    - 注意点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# データ生成\n",
    "X, y = make_gaussian_quantiles(n_features=2, n_classes=2, n_samples=300)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "model = SVC()#デフォルトカーネル:RBF\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. NaiveBayes  \n",
    "    - 方法\n",
    "    - 得意領域\n",
    "    - 注意点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Random Forest  \n",
    "    回帰/分類\n",
    "    - 方法  \n",
    "        複数の決定木を組み合わせる\n",
    "        - 決定木学習  \n",
    "            ジニ係数などの不純度が小さくなるようにデータを分割\n",
    "    $$\n",
    "    ジニ係数=1-\\sum_{i=1}^{c}p_i^2 \\quad\n",
    "    $$\n",
    "        - ランダムフォレストの特徴  \n",
    "            ブーストラップ法でデータを水増し、特徴量をランダムに選び、多様性を持った決定木学習       \n",
    "    - 得意領域\n",
    "    - 注意点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# データ読み込み\n",
    "data = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3)\n",
    "\n",
    "model = RandomForestClassifier() \n",
    "model.fit(X_train, y_train) # 学習\n",
    "y_pred = model.predict(X_test) \n",
    "accuracy_score(y_pred, y_test) # 評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Neural Network  \n",
    "    - 方法  \n",
    "        - アーリーストッピング  \n",
    "            train-dataを更に検証用に分け、検証データで過学習になったら学習をやめる\n",
    "    - 得意領域\n",
    "    - 注意点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. kNN(k-Nearest Neighbor method)  \n",
    "    回帰/分類\n",
    "    - 方法  \n",
    "        入力データに近いk個の学習データから多数決で分類する\n",
    "    - 得意領域\n",
    "    - 注意点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# データ生成\n",
    "X, y = make_moons(noise=0.3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "model = KNeighborsClassifier() \n",
    "model.fit(X_train, y_train) # 学習\n",
    "y_pred = model.predict(X_test) \n",
    "accuracy_score(y_pred, y_test) # 評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. PCA(Principal Component Analysis)\n",
    "\n",
    "    次元削減、低次元の方向と重要度を見つける。\n",
    "    - 方法\n",
    "    分散共分散行列をもとに固有値問題を解く。\n",
    "    主成分ごとの固有値を固有値の総和で割ったものを寄与率とする。\n",
    "    寄与率:主成分が元データ情報の説明率\n",
    "    - 得意領域\n",
    "\n",
    "    - 注意点\n",
    "    累積寄与率が急激に上がらなければPCAは不適切。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "data = load_iris()\n",
    "model = PCA(n_components=2)\n",
    "model = model.fit(data.data)\n",
    "\n",
    "print(model.transform(data.data))# 変換後データ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. LSA(Latent Semantic Analysis)  \n",
    "    次元削減、文章や単語の行列を潜在的な意味空間へ変換する。\n",
    "    - 方法\n",
    "    - 得意領域\n",
    "    - 注意点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "model = TruncatedSVD(n_components=2)\n",
    "model.fit(data)\n",
    "\n",
    "print(model.transform(data))# 変換後データ\n",
    "print(model.explained_variance_ratio_)# 寄与率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. NMF(Non-negative Matrix Factorization)   \n",
    "    次元削減、元データを2つの行列に分解する。(W,H)\n",
    "    行列要素が非負で、分解後も非負/変数が直行しない(⇔PCA)\n",
    "    - 方法\n",
    "    - 得意領域\n",
    "    - 注意点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "centers = [[5, 10, 5], [10, 4, 10], [6, 8, 8]]\n",
    "X, _ = make_blobs(centers=centers) # centersを中心としたデータ生成\n",
    "\n",
    "model = NMF(n_components=2)# 潜在変数の数\n",
    "model.fit(X)\n",
    "W = model.transform(X) # 分解後の行列\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. LDA(Latent Dirichlet allocation)  \n",
    "    次元削減、トピックモデル、PLSIのベイズ化モデル\n",
    "    - 方法  \n",
    "        各単語がどのトピックから生成されたかに関する確率分布を求める。\n",
    "    $$\n",
    "        p(θ,z|w,α,β)=\\frac{p(θ,z,w|α,β)}{p(w|α,β)}\n",
    "    $$\n",
    "        p(w|α,β)は出せないので、以下で近似。\n",
    "\n",
    "    $$\n",
    "        q(θ,z|γ,φ)=q(θ|φ)\\prod_{n=1}{N}q(z_n|φ_n)\n",
    "    $$\n",
    "\n",
    "        q(θ|γ)がディリクレ分布(パラメータ:トピック数)  \n",
    "        q(z|φ)が多項分布(パラメータ:単語数×トピック数)\n",
    "\n",
    "    - 得意領域\n",
    "    - 注意点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-874561a81c04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_20newsgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    " \n",
    "\n",
    "# removeで本文以外の情報を取り除く\n",
    "data = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))\n",
    "max_features = 1000\n",
    "# 文書 データをベクトルに変換\n",
    "tf_vectorizer = CountVectorizer(max_features=max_features,\n",
    "stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(data.data)\n",
    "n_topics = 20\n",
    "model = LatentDirichletAllocation(n_components=n_topics)\n",
    "model.fit(tf)\n",
    "print(model.components_) # 各トピックが持つ単語分布 \n",
    "print(model.transform(tf)) # トピックで表現された文書"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. k-means method  \n",
    "    クラスタリング\n",
    "    - 方法  \n",
    "        クラスタ重心を決め、距離でクラスタ分類する。  \n",
    "        クラスタ平均が重心と同じになるまで、クラスタ平均で分類を繰り返す。\n",
    "    - 得意領域\n",
    "    - 注意点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "model = KMeans(n_clusters=3) # クラスタ数を3に設定\n",
    "model.fit(data.data)\n",
    "print(model.labels_) # 各データ点が所属するクラスタ \n",
    "print(model.cluster_centers_) # fit()によって計算された重心"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. GMM(Gaussian Mixture Model)  \n",
    "    クラスタリング、複数の正規分布で評価\n",
    "    - 方法\n",
    "    - 得意領域\n",
    "    - 注意点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "data = load_iris()\n",
    "\n",
    "model = GaussianMixture(n_components=3) # ガウス分布の数\n",
    "model.fit(data.data) \n",
    "print(model.predict(data.data)) # クラスを予測 \n",
    "print(model.means_) # 各ガウス分布の平均 \n",
    "print(model.covariances_) # 各ガウス分布の分散"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. LLE(Local Linear Embedding)  \n",
    "    次元削除、多面体学習、k-meansの次元削除版\n",
    "    - 方法\n",
    "    - 得意領域  \n",
    "        ねじれ、曲がった高次元データ\n",
    "    - 注意点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import samples_generator\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "\n",
    "data, color = samples_generator.make_swiss_roll(n_samples=1500)\n",
    "n_neighbors = 12 # 近傍点の数 \n",
    "n_components = 2 # 削減後の次元数\n",
    "model = LocallyLinearEmbedding(n_neighbors=n_neighbors,\n",
    "n_components=n_components)\n",
    "model.fit(data)\n",
    "print(model.transform(data)) # 変換したデータ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. t-SNE(t-Distributed Stochastic Neighbor Embedding)  \n",
    "    次元削減、多様体学習、自由度1のt分布\n",
    "    - 方法\n",
    "    - 得意領域\n",
    "    バラけた三次元データ\n",
    "    - 注意点\n",
    "    4次元以上は精度でないかも"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.manifold import TSNE\n",
    "import sklearn.datasets import load_digits\n",
    "data = load_digits()\n",
    "model = TSNE(n_components=2)#削減後次元\n",
    "model.fit(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
